{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/momchil/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library versions:\n",
      "tensorflow:1.7.0\n",
      "pandas:0.22.0\n",
      "numpy:1.14.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713e33ab084743ca865de0ccf7f33326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import importlib\n",
    "\n",
    "import customersupport.common\n",
    "import customersupport.common.utils\n",
    "import customersupport.evaluation.eval\n",
    "\n",
    "print('Library versions:')\n",
    "\n",
    "import tensorflow as tf\n",
    "print('tensorflow:{}'.format(tf.__version__))\n",
    "import pandas as pd\n",
    "print('pandas:{}'.format(pd.__version__))\n",
    "# import sklearn\n",
    "# print('sklearn:{}'.format(sklearn.__version__))\n",
    "# import nltk\n",
    "# print('nltk:{}'.format(nltk.__version__))\n",
    "import numpy as np\n",
    "print('numpy:{}'.format(np.__version__))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import SVG\n",
    "from tqdm import tqdm_notebook as tqdm  # Special jupyter notebook progress bar\n",
    "\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from datetime import datetime\n",
    "\n",
    "from customersupport.common.vocab import VocabHolder\n",
    "from customersupport.common.dataset import CustomerSupportDataset\n",
    "\n",
    "from customersupport.evaluation.eval import evaluate_words_index, format_metrics, get_evaluation_conf, strip_punkt\n",
    "\n",
    "importlib.reload(customersupport.common.vocab)\n",
    "importlib.reload(customersupport.common.dataset)\n",
    "importlib.reload(customersupport.common.utils)\n",
    "importlib.reload(customersupport.evaluation.eval)\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "tqdm().pandas()  # Enable tracking of progress in dataframe `apply` calls\n",
    "\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.physical_device_desc for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/momchil/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# 8192 - large enough for demonstration, larger values make network training slower\n",
    "MAX_VOCAB_SIZE = 2**12\n",
    "\n",
    "# seq2seq generally relies on fixed length message vectors - longer messages provide more info\n",
    "# but result in slower training and larger networks\n",
    "#MAX_MESSAGE_LEN = 50\n",
    "\n",
    "hparams = tf.contrib.training.HParams(\n",
    "    # Larger batch sizes generally reach the average response faster, but small batch sizes are\n",
    "    # required for the model to learn nuanced responses.  Also, GPU memory limits max batch size.\n",
    "    batch_size=128,\n",
    "    encoder_length=60,\n",
    "    decoder_length=70,\n",
    "    # Embedding size for whole messages, same trade off as word embeddings\n",
    "    num_units=256,\n",
    "    src_vocab_size=MAX_VOCAB_SIZE,\n",
    "    # Embedding size for words - gives a trade off between expressivity of words and network size\n",
    "    embedding_size=200,\n",
    "    tgt_vocab_size=MAX_VOCAB_SIZE,\n",
    "    # Helps regularize network and prevent overfitting.\n",
    "    # High learning rate helps model reach average response faster, but can make it hard to\n",
    "    # converge on nuanced responses\n",
    "    learning_rate=0.001,  #0.0005,\n",
    "    max_gradient_norm=5.0,\n",
    "    beam_width=10,\n",
    "    use_attention=True,\n",
    "    enc_num_layers=4,\n",
    "    dec_num_layers=4,\n",
    "    cell_type='bi',\n",
    "    rnn_type='lstm',\n",
    "    attention_architecture='gnmt',\n",
    "    max_epochs=17,\n",
    "    dropout=0.2,\n",
    "    use_glove=True,\n",
    "    l2_reg=0.,\n",
    "    glove_path=\n",
    "    '/home/momchil/Storage/Projects/Python/Data/glove.twitter.27B/glove.twitter.27B.200d.txt',\n",
    "    tweets_path=\n",
    "    '/home/momchil/Storage/Projects/Python/Data/customer-support-on-twitter/twcs-conv_ids_clean.csv',\n",
    "    # Ngram count for ROUGE and BLEU\n",
    "    max_order=2,\n",
    "    train_size=0.8,\n",
    "    decay_rate=0.9,\n",
    "    train_time_diff=5.0,\n",
    "    first_day=0,\n",
    "    last_day=60,\n",
    "    evaluation_metrics=[\n",
    "        \"bleu\", \"rouge_l\", \"embedding_average\", \"vector_extrema\",\n",
    "        \"greedy_matching\"\n",
    "    ],\n",
    "    training_metrics=[\n",
    "        \"bleu\", \"rouge_l\", \"embedding_average\", \"vector_extrema\",\n",
    "        \"greedy_matching\"\n",
    "    ],\n",
    "    companies=['AppleSupport'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Initializations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_id_chunks(data_len, batch_size):\n",
    "    iters_per_epoch =  int(np.ceil(data_len / batch_size))\n",
    "    \n",
    "    batch_ids = np.array(list(range(data_len)))\n",
    "    #np.random.shuffle(batch_ids)\n",
    "\n",
    "    batch_ids = np.split(batch_ids, ([hparams.batch_size * i for i in range(1, iters_per_epoch)]))\n",
    "    \n",
    "    return batch_ids\n",
    "\n",
    "def next_batch(x, y, train, \n",
    "               rand_idx = None, \n",
    "               weights = None, \n",
    "               beam_width = 10, \n",
    "               batch_size = 128):\n",
    "    if rand_idx is None:\n",
    "        rand_idx = random.sample(list(range(len(x))), batch_size)\n",
    "    batch_size = len(rand_idx)\n",
    "    enc, dec, weights = customersupport.common.utils.transform_batch(x, y, rand_idx, weights=weights, \n",
    "                                                                     batch_size=batch_size, \n",
    "                                                                     mask_pads=True)\n",
    "    seq_lengths = np.count_nonzero(enc[0].T, axis=0)\n",
    "    dec_seq_lengths = np.count_nonzero(dec, axis=1)\n",
    "    feed_dict = {\n",
    "        encoder_inputs: enc[0],\n",
    "        source_sequence_length: seq_lengths,\n",
    "        target_labels: dec,\n",
    "        decoder_inputs: enc[1].T,\n",
    "        decoder_lengths: dec_seq_lengths,\n",
    "        target_weights: weights,\n",
    "        batch_size_tensor: batch_size,\n",
    "        beam_width_tensor: 1 if train else beam_width,\n",
    "        keep_prob: (1 - hparams.dropout) if (train) else 1.\n",
    "    }\n",
    "\n",
    "    return feed_dict\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def embed_tensor(inputs, pretrained_embs, name, trainable=False):\n",
    "\n",
    "    train_embeddings = tf.get_variable(\n",
    "        name=name,\n",
    "        shape=[hparams.tgt_vocab_size - len(voc_holder.glove_weights), hparams.embedding_size],\n",
    "        initializer=tf.random_uniform_initializer(-0.04, 0.04),\n",
    "        trainable=True)\n",
    "    \n",
    "    embeddings = tf.concat(\n",
    "        [train_embeddings, pretrained_embs], axis=0)\n",
    "\n",
    "    return tf.nn.embedding_lookup(embeddings, inputs), embeddings\n",
    "\n",
    "def get_references_and_hypothesis(x, y, eval_conf, verbose):\n",
    "    references = []\n",
    "    hypothesis = []\n",
    "    losses = []\n",
    "    \n",
    "    batch_size = eval_conf.batch_size\n",
    "    beam_width = eval_conf.beam_width\n",
    "    \n",
    "    batch_ids = get_batch_id_chunks(len(x), batch_size)\n",
    "\n",
    "    gen = range(len(batch_ids))\n",
    "    \n",
    "    if (verbose):\n",
    "        gen = tqdm(gen)\n",
    "    \n",
    "    for i in gen:\n",
    "        feed_dict = next_batch(x, y, False,\n",
    "            rand_idx=batch_ids[i], \n",
    "            weights=None,\n",
    "            beam_width=beam_width)\n",
    "        \n",
    "        responses = sess.run(eval_conf.seq_func, feed_dict=feed_dict)\n",
    "        feed_dict[beam_width_tensor] = 1\n",
    "        loss = sess.run(train_loss, feed_dict=feed_dict)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        if (len(responses.shape) > 2):\n",
    "            responses = responses[0]\n",
    "            \n",
    "        for (h, r) in zip(responses, feed_dict.get(target_labels)):\n",
    "            #references.append(strip_punkt(r, eval_conf.voc_holder.reverse_vocab))\n",
    "            #hypothesis.append(strip_punkt(h, eval_conf.voc_holder.reverse_vocab))\n",
    "            references.append(r[r.nonzero()])\n",
    "            hypothesis.append(h[h.nonzero()])\n",
    "\n",
    "    return (np.array(references), np.array(hypothesis), np.array(losses))\n",
    "\n",
    "def evaluate(x, y, eval_conf, metrics, verbose = False):\n",
    "    references, hypothesis, losses = get_references_and_hypothesis(x, y, eval_conf, verbose)\n",
    "    \n",
    "    evaluation = evaluate_words_index(references, hypothesis, eval_conf, metrics, verbose)\n",
    "    \n",
    "    loss_value = np.mean(losses)\n",
    "    perplexity = np.exp(loss_value / eval_conf.decoder_length)\n",
    "    \n",
    "    evaluation['Loss'] = loss_value\n",
    "    evaluation['Perplexity'] = perplexity\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cs_data = CustomerSupportDataset(hparams)\n",
    "\n",
    "#& (y_text.str.contains('help') ^ True)\n",
    "#['direct message', 'is fixed in a future software update']\n",
    "cs_data.process_utterances(['direct message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_holder = VocabHolder(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = voc_holder.fit(cs_data.x_text, cs_data.y_text, hparams.src_vocab_size)\n",
    "\n",
    "cs_data.text_to_vec(hparams, voc_holder)\n",
    "cs_data.train_test_split(hparams, do_random=False)\n",
    "\n",
    "train_x = cs_data.train_x\n",
    "train_y = cs_data.train_y\n",
    "\n",
    "test_x = cs_data.test_x\n",
    "test_y = cs_data.test_y\n",
    "\n",
    "train_weights = cs_data.train_weights\n",
    "test_weights = cs_data.test_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure()\n",
    "_= plt.hist(cs_data.inbounds_and_outbounds.time_diff.values, 6, range=[0, 60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "#Source: https://github.com/tensorflow/nmt/blob/master/nmt/gnmt_model.py\n",
    "\n",
    "class GNMTAttentionMultiCell(tf.nn.rnn_cell.MultiRNNCell):\n",
    "    \"\"\"A MultiCell with GNMT attention style.\"\"\"\n",
    "\n",
    "    def __init__(self, attention_cell, cells, use_new_attention=False):\n",
    "        \"\"\"Creates a GNMTAttentionMultiCell.\n",
    "        Args:\n",
    "          attention_cell: An instance of AttentionWrapper.\n",
    "          cells: A list of RNNCell wrapped with AttentionInputWrapper.\n",
    "          use_new_attention: Whether to use the attention generated from current\n",
    "            step bottom layer's output. Default is False.\n",
    "        \"\"\"\n",
    "        cells = [attention_cell] + cells\n",
    "        self.use_new_attention = use_new_attention\n",
    "        super(GNMTAttentionMultiCell, self).__init__(cells, state_is_tuple=True)\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Run the cell with bottom layer's attention copied to all upper layers.\"\"\"\n",
    "        if not nest.is_sequence(state):\n",
    "            raise ValueError(\n",
    "              \"Expected state to be a tuple of length %d, but received: %s\"\n",
    "              % (len(self.state_size), state))\n",
    "\n",
    "        new_states = []\n",
    "\n",
    "        attention_cell = self._cells[0]\n",
    "        attention_state = state[0]\n",
    "        cur_inp, new_attention_state = attention_cell(inputs, attention_state)\n",
    "        new_states.append(new_attention_state)\n",
    "\n",
    "        for i in range(1, len(self._cells)):\n",
    "            cell = self._cells[i]\n",
    "            cur_state = state[i]\n",
    "\n",
    "            if self.use_new_attention:\n",
    "                cur_inp = tf.concat([cur_inp, new_attention_state.attention], -1)\n",
    "            else:\n",
    "                cur_inp = tf.concat([cur_inp, attention_state.attention], -1)\n",
    "\n",
    "            cur_inp, new_state = cell(cur_inp, cur_state)\n",
    "            new_states.append(new_state)\n",
    "\n",
    "        return cur_inp, tuple(new_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(beam_width):\n",
    "    # Replicate encoder infos beam_width times\n",
    "#     decoder_initial_state = (decoder_cell\n",
    "#                                  .zero_state(batch_size_tensor * beam_width, tf.float32)\n",
    "#                                  .clone(cell_state=encoder_state))\n",
    "#    decoder_initial_state = initial_state\n",
    "    # Define a beam-search decoder\n",
    "    decoder_beam = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "            cell=decoder_cell,\n",
    "            embedding=embedding_decoder,\n",
    "            start_tokens=tf.fill([batch_size_tensor], tgt_sos_id),\n",
    "            end_token=tgt_eos_id,\n",
    "            initial_state=initial_state,\n",
    "            beam_width=beam_width,\n",
    "            output_layer=projection_layer,\n",
    "            length_penalty_weight=0.0)\n",
    "\n",
    "\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder_beam, maximum_iterations=maximum_iterations,\n",
    "        parallel_iterations=128)\n",
    "    \n",
    "    translations = tf.transpose(outputs.predicted_ids, [2, 0, 1])\n",
    "    \n",
    "    return translations\n",
    "\n",
    "def greedy_decoder():\n",
    "    # Inference\n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "        embedding_decoder, tf.fill([batch_size_tensor], tgt_sos_id),\n",
    "        tgt_eos_id)\n",
    "\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell,\n",
    "        inference_helper,\n",
    "        initial_state,\n",
    "        output_layer=projection_layer)\n",
    "    # Dynamic decoding\n",
    "    outputs, final_context_state, _ = \\\n",
    "        tf.contrib.seq2seq.dynamic_decode(decoder, maximum_iterations=maximum_iterations)\n",
    "    translations = outputs.sample_id\n",
    "\n",
    "    return translations\n",
    "\n",
    "def _rnn_cell(num_units, keep_prob, state_is_tuple = True):\n",
    "    #initializer=tf.glorot_uniform_initializer()\n",
    "    \n",
    "    #Luong init https://arxiv.org/pdf/1508.04025.pdf \n",
    "    initializer=tf.random_uniform_initializer(-0.1, 0.1)\n",
    "    \n",
    "    _cell = (tf.nn.rnn_cell.GRUCell(num_units, kernel_initializer=initializer) if (hparams.rnn_type == 'gru') else \n",
    "            tf.nn.rnn_cell.LSTMCell(num_units, state_is_tuple=state_is_tuple, initializer=initializer))\n",
    "    _cell = tf.contrib.rnn.DropoutWrapper(_cell, output_keep_prob=keep_prob, variational_recurrent=True, dtype=tf.float32)\n",
    "    return _cell\n",
    "\n",
    "def get_cell_list(num_units, keep_prob, state_is_tuple = True, layers = 1):\n",
    "    return [_rnn_cell(num_units, keep_prob, state_is_tuple) for i in range(layers)]\n",
    "\n",
    "def build_encoder_cell(num_units, keep_prob, state_is_tuple = True, layers = 1):\n",
    "    cell_list = get_cell_list(num_units, keep_prob, state_is_tuple, layers)\n",
    "    \n",
    "    if (layers == 1):\n",
    "        cell = cell_list[0]\n",
    "    else:\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cell_list, state_is_tuple=state_is_tuple)\n",
    "        \n",
    "    return cell\n",
    "    \n",
    "    \n",
    "\n",
    "def build_decoder_cell(num_units,\n",
    "                   keep_prob,\n",
    "                   attention_states=None,\n",
    "                   state_is_tuple=True,\n",
    "                   layers=1):\n",
    "    \n",
    "    cell_list = [\n",
    "        _rnn_cell(num_units, keep_prob, state_is_tuple)\n",
    "        for i in range(layers)\n",
    "    ]\n",
    "    \n",
    "    if (hparams.use_attention):\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "            hparams.num_units,\n",
    "            attention_states,\n",
    "            memory_sequence_length=tiled_sequence_length,\n",
    "            scale=True)\n",
    "        \n",
    "        if (\"gnmt\" in hparams.attention_architecture):\n",
    "            # Only wrap the bottom layer with the attention mechanism.\n",
    "            decoder_cell = cell_list.pop(0)\n",
    "            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                decoder_cell,\n",
    "                attention_mechanism,\n",
    "                attention_layer_size=None,  # don't use attention layer.\n",
    "                output_attention=False,\n",
    "                alignment_history=False,\n",
    "                name=\"attention\")\n",
    "\n",
    "            decoder_cell = GNMTAttentionMultiCell(\n",
    "                decoder_cell,\n",
    "                cell_list,\n",
    "                use_new_attention=(hparams.attention_architecture == \"gnmt_v2\"))\n",
    "        else:\n",
    "            decoder_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                cell_list, state_is_tuple=state_is_tuple)\n",
    "            \n",
    "            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                decoder_cell,\n",
    "                attention_mechanism,\n",
    "                attention_layer_size=hparams.num_units) \n",
    "            \n",
    "    return decoder_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repos\n",
    "https://gist.github.com/higepon/eb81ba0f6663a57ff1908442ce753084\n",
    "\n",
    "https://github.com/tensorflow/nmt/blob/master/nmt/model.py\n",
    "\n",
    "https://github.com/tensorflow/nmt/blob/master/nmt/attention_model.py\n",
    "\n",
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Symbol for start decode process.\n",
    "tgt_sos_id = customersupport.common.utils.START\n",
    "\n",
    "# Symbol for end of decode process.\n",
    "tgt_eos_id = customersupport.common.utils.PAD\n",
    "\n",
    "# For debug purpose.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# We should specify maximum_iterations, it can't stop otherwise.\n",
    "# maximum_iterations = tf.round(tf.reduce_max(hparams.decoder_length))\n",
    "\n",
    "# Encoder\n",
    "#   encoder_inputs: [encoder_length, batch_size]\n",
    "#   This is time major where encoder_length comes first instead of batch_size.\n",
    "\n",
    "beam_width_tensor = tf.placeholder(\n",
    "    tf.int32, shape=(), name=\"beam_width\")\n",
    "\n",
    "batch_size_tensor = tf.placeholder(\n",
    "    tf.int32, shape=(), name=\"batch_size\")\n",
    "\n",
    "keep_prob = tf.placeholder_with_default(1.0, shape=())\n",
    "\n",
    "encoder_inputs = tf.placeholder(\n",
    "    tf.int32, shape=(None, None), name=\"encoder_inputs\")\n",
    "\n",
    "source_sequence_length = tf.placeholder(tf.int32, [None], name=\"encoder_lengths\")\n",
    "\n",
    "# We should specify maximum_iterations, it can't stop otherwise.\n",
    "maximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)\n",
    "\n",
    "tiled_sequence_length = tf.contrib.seq2seq.tile_batch(\n",
    "    source_sequence_length, multiplier=beam_width_tensor)\n",
    "\n",
    "if (hparams.use_glove):\n",
    "    # define the word embedding\n",
    "    pretrained_embs = tf.get_variable(\n",
    "        name=\"embs_pretrained\",\n",
    "        initializer=tf.constant_initializer(\n",
    "           voc_holder.glove_weights, dtype=tf.float32),\n",
    "        shape=voc_holder.glove_weights.shape,\n",
    "        trainable=False)\n",
    "\n",
    "# Look up embedding:\n",
    "#   encoder_inputs: [encoder_length, batch_size]\n",
    "#   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
    "if (hparams.use_glove):\n",
    "    encoder_emb_inputs, embedding_encoder = embed_tensor(encoder_inputs, pretrained_embs, \"embedding_encoder_glove\")\n",
    "else:\n",
    "    # Embedding\n",
    "    #   Matrix for embedding: [src_vocab_size, embedding_size]\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", [hparams.src_vocab_size, hparams.embedding_size])\n",
    "    encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder, encoder_inputs)\n",
    "\n",
    "if (hparams.cell_type == 'bi'):\n",
    "    num_bi_layers = max(hparams.enc_num_layers // 2, 1)\n",
    "\n",
    "    # LSTM cell.\n",
    "    # Construct forward and backward cells\n",
    "    fw_cell = build_encoder_cell(hparams.num_units, keep_prob, layers = num_bi_layers)\n",
    "    bw_cell = build_encoder_cell(hparams.num_units, keep_prob, layers = num_bi_layers)\n",
    "\n",
    "    bi_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        fw_cell, bw_cell, encoder_emb_inputs,\n",
    "        sequence_length=source_sequence_length, time_major=False, dtype=tf.float32)\n",
    "    encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "    \n",
    "    if num_bi_layers == 1:\n",
    "        encoder_state = bi_encoder_state\n",
    "    else:\n",
    "        # alternatively concat forward and backward states\n",
    "        encoder_state = []\n",
    "        for layer_id in range(num_bi_layers):\n",
    "            encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "            encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "\n",
    "    encoder_state = tuple(encoder_state)\n",
    "else:\n",
    "    fw_cell = build_encoder_cell(hparams.num_units, keep_prob, layers = hparams.enc_num_layers)\n",
    "\n",
    "    # Run Dynamic RNN\n",
    "    #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
    "    #   encoder_state: [batch_size, num_units], this is final state of the cell for each batch.\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "        fw_cell, encoder_emb_inputs, time_major=False, \n",
    "        sequence_length= source_sequence_length, \n",
    "        dtype=tf.float32)\n",
    "\n",
    "encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, beam_width_tensor)\n",
    "encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, beam_width_tensor)\n",
    "    \n",
    "decoder_inputs = tf.placeholder(\n",
    "    tf.int32, shape=(hparams.decoder_length + 1, None), name=\"decoder_inputs\")\n",
    "decoder_lengths = tf.placeholder(\n",
    "    tf.int32, shape=[None], name=\"decoder_lengths\")\n",
    "\n",
    "if (hparams.use_glove):\n",
    "    decoder_emb_inputs, embedding_decoder  = embed_tensor(decoder_inputs, pretrained_embs, \"embedding_decoder_glove\")\n",
    "else:\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", [hparams.tgt_vocab_size, hparams.embedding_size])\n",
    "    decoder_emb_inputs = tf.nn.embedding_lookup(embedding_decoder, decoder_inputs)\n",
    "\n",
    "projection_layer = layers_core.Dense(hparams.tgt_vocab_size, use_bias=False)\n",
    "\n",
    "decoder_cell = build_decoder_cell(hparams.num_units, keep_prob, encoder_outputs, layers=hparams.dec_num_layers)\n",
    "\n",
    "initial_state = decoder_cell.zero_state(batch_size_tensor * beam_width_tensor, tf.float32)\n",
    "\n",
    "if ((hparams.dec_num_layers == hparams.enc_num_layers) and (hparams.use_attention) and \n",
    "    ('gnmt' not in hparams.attention_architecture)):\n",
    "    print('Copying state')\n",
    "    initial_state = initial_state.clone(cell_state=encoder_state)\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_emb_inputs, tf.ones(batch_size_tensor, tf.int32) * hparams.decoder_length, time_major=True)\n",
    "\n",
    "# Decoder\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper, initial_state)\n",
    "    #output_layer=projection_layer)\n",
    "# Dynamic decoding\n",
    "final_outputs, _final_state, _final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, maximum_iterations=maximum_iterations)\n",
    "\n",
    "logits = projection_layer(final_outputs.rnn_output)\n",
    "\n",
    "target_labels = tf.placeholder(tf.int32, shape=(None, hparams.decoder_length))\n",
    "\n",
    "# Masking weights\n",
    "target_weights = tf.placeholder(\n",
    "    tf.float32, shape=(None, hparams.decoder_length), name=\"target_weights\")\n",
    "\n",
    "# Loss\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=target_labels, logits=logits)\n",
    "\n",
    "accuracy = tf.reduce_mean(\n",
    "     tf.divide(\n",
    "         tf.reduce_sum(tf.cast(tf.equal(tf.cast(tf.argmax(logits, 2), tf.int32), target_labels), tf.float32) * target_weights, 1),\n",
    "         tf.cast(decoder_lengths, tf.float32)\n",
    "     ),\n",
    "     name='acc')\n",
    "\n",
    "train_loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "    tf.cast(batch_size_tensor, tf.float32))\n",
    "\n",
    "l2 = hparams.l2_reg * sum(\n",
    "    tf.nn.l2_loss(tf_var)\n",
    "        for tf_var in tf.trainable_variables()\n",
    "        if not (\"noreg\" in tf_var.name or \"bias\" in tf_var.name)\n",
    ")\n",
    "train_loss += l2\n",
    "\n",
    "# Train\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Calculate and clip gradients\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(train_loss, params)\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients,\n",
    "                                              hparams.max_gradient_norm)\n",
    "\n",
    "iters_per_epoch = int(np.ceil(train_x.shape[0] / hparams.batch_size))\n",
    "\n",
    "# Optimization\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    hparams.learning_rate, global_step, iters_per_epoch, .99, staircase=True)\n",
    "#learning_rate = tf.Variable(1., dtype=np.float32)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(\n",
    "    zip(clipped_gradients, params), global_step=global_step)\n",
    "\n",
    "beam_translations = beam_search_decoder(hparams.beam_width)\n",
    "greedy_translations = greedy_decoder()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('cross_entropy', train_loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "merged = tf.summary.merge_all()\n",
    "now = str(int(datetime.now().timestamp()))\n",
    "summaries_dir = './logs'\n",
    "train_writer = tf.summary.FileWriter(summaries_dir + '/train' + now,\n",
    "                                      sess.graph)\n",
    "test_writer = tf.summary.FileWriter(summaries_dir + '/test' + now)\n",
    "print('Model id #{}'.format(now))\n",
    "\n",
    "eval_conf = get_evaluation_conf(sess, hparams, beam_translations, train_loss, voc_holder)\n",
    "eval_conf_greedy = get_evaluation_conf(sess, hparams, greedy_translations, train_loss, voc_holder)\n",
    "eval_conf_greedy.beam_width = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "try:\n",
    "    for epoch in range(hparams.max_epochs):\n",
    "        print('\\nStarting epoch = {}/{}'.format(epoch + 1, hparams.max_epochs))\n",
    "        batch_ids = get_batch_id_chunks(train_x.shape[0], hparams.batch_size)\n",
    "\n",
    "        lr = sess.run(learning_rate)\n",
    "        loss = .0\n",
    "        \n",
    "        perc = 0\n",
    "        t_start = time.time()\n",
    "        \n",
    "        iters_per_epoch = len(batch_ids)\n",
    "        for i in tqdm(range(iters_per_epoch)):\n",
    "            try:\n",
    "                step = sess.run(global_step)\n",
    "                feed_dict = next_batch(train_x, train_y, True, \n",
    "                                       rand_idx=batch_ids[i],\n",
    "                                       weights=None,\n",
    "                                       beam_width=1)\n",
    "                _, loss_value, summary = sess.run([train_op, train_loss, merged], feed_dict=feed_dict)\n",
    "                loss += loss_value\n",
    "                train_writer.add_summary(summary, step)\n",
    "\n",
    "                if (i % 10 == 0):\n",
    "                    feed_dict = next_batch(test_x, test_y, False, \n",
    "                                           beam_width=1, \n",
    "                                           batch_size=hparams.batch_size)\n",
    "                    summary = sess.run(merged, feed_dict=feed_dict)\n",
    "                    test_writer.add_summary(summary, step)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(batch_ids[i])\n",
    "                print(feed_dict)\n",
    "\n",
    "        t_epoch_end = time.time() - t_start\n",
    "\n",
    "        t_start = time.time()\n",
    "        \n",
    "        evaluation = evaluate(test_x, test_y, eval_conf_greedy, hparams.training_metrics, True)\n",
    "        evaluation['Train Loss'] = loss / iters_per_epoch\n",
    "        \n",
    "        t_test_end = time.time() - t_start\n",
    "        \n",
    "        print(format_metrics(evaluation))\n",
    "        print()\n",
    "        print('Train elapsed {}, Test elapsed {}'.format(t_epoch_end, t_test_end))\n",
    "        print('Updating lr = {}'.format(lr))\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nHalting training from keyboard interrupt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = evaluate(test_x, test_y, eval_conf, hparams.evaluation_metrics, True)\n",
    "print(format_metrics(evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 20\n",
    "beam_width = hparams.beam_width\n",
    "\n",
    "feed_dict = next_batch(test_x, test_y, False, beam_width=beam_width, batch_size=batch)\n",
    "\n",
    "feed_dict[beam_width_tensor] = 1\n",
    "replies_greedy = sess.run(greedy_translations, feed_dict=feed_dict)\n",
    "\n",
    "feed_dict[beam_width_tensor] = beam_width\n",
    "replies_beam = (['\\n'.join([voc_holder.from_word_idx(u)for u in row]) \n",
    "                for row in np.transpose(sess.run(beam_translations, feed_dict=feed_dict), [1, 0, 2])])\n",
    "\n",
    "for i in range(batch):\n",
    "    print('*' * 50 + 'Dialog #{}'.format(i + 1) + '*' * 50)\n",
    "    print('>>>>> ', end='')\n",
    "    print(voc_holder.from_word_idx(feed_dict.get(encoder_inputs).T[i]))\n",
    "    print('<<<<< ', end='')\n",
    "    print(voc_holder.from_word_idx(feed_dict.get(target_labels)[i]))\n",
    "    print()\n",
    "    print('Greedy: ' + voc_holder.from_word_idx(np.array(replies_greedy[i])))\n",
    "    if (replies_beam is not None):\n",
    "        print('Beam: \\n' +  replies_beam[i])\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = evaluate(train_x, train_y, eval_conf, hparams.evaluation_metrics, True)\n",
    "print(format_metrics(evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver = tf.train.Saver()\n",
    "#saver.save(sess, \"/home/momchil/Storage/Projects/Python/Customer Support/models/1521915353.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(x, y, eval_conf):\n",
    "    verbose = True\n",
    "    references = []\n",
    "    hypothesis = []\n",
    "    losses = []\n",
    "    questions = []\n",
    "\n",
    "    batch_size = eval_conf.batch_size\n",
    "    beam_width = eval_conf.beam_width\n",
    "\n",
    "    batch_ids = get_batch_id_chunks(len(x), batch_size)\n",
    "\n",
    "    gen = range(len(batch_ids))\n",
    "    exclude = set(string.punctuation)\n",
    "\n",
    "    if (verbose):\n",
    "        gen = tqdm(gen)\n",
    "\n",
    "    for i in gen:\n",
    "        feed_dict = next_batch(x, y, False,\n",
    "            rand_idx=batch_ids[i], \n",
    "            weights=None,\n",
    "            beam_width=beam_width)\n",
    "        \n",
    "        responses = sess.run(eval_conf.seq_func, feed_dict=feed_dict)[0]\n",
    "        feed_dict[beam_width_tensor] = 1\n",
    "        loss = sess.run(tf.reduce_mean(crossent * target_weights, 1), feed_dict=feed_dict)\n",
    "\n",
    "        for (h, r, q, l) in zip(responses, feed_dict.get(target_labels), feed_dict.get(encoder_inputs), loss):\n",
    "            references.append(voc_holder.from_word_idx(strip_punkt(r, eval_conf.voc_holder.reverse_vocab)))\n",
    "            hypothesis.append(voc_holder.from_word_idx(strip_punkt(h, eval_conf.voc_holder.reverse_vocab)))\n",
    "            questions.append(voc_holder.from_word_idx(q))\n",
    "            losses.append(l)\n",
    "        print('{}'.format(len(losses)), end='...')\n",
    "            \n",
    "    return (np.array(questions), np.array(references), np.array(hypothesis), np.array(losses))\n",
    "\n",
    "(q, r, h, l) = export(test_x, test_y, eval_conf)\n",
    "df = pd.DataFrame(data = np.vstack((q, r, h, l)).T, \n",
    "                 columns = [\"Question\", \"Reference\", \"Hypothesis\", \"Loss\" ])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/momchil/Desktop/seq2seq_all_dict.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_glove_vec(words):\n",
    "    glove_vecs = np.zeros(200)\n",
    "   \n",
    "    gen = (r for r in words if r > 0)\n",
    "    \n",
    "    i = 0.\n",
    "    for r in gen:\n",
    "        try:\n",
    "            glove_vecs += glove_weights[r]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        i += 1.\n",
    "    \n",
    "    return glove_vecs / i\n",
    "\n",
    "def to_binary_vec(words):\n",
    "    glove_vecs = np.zeros(hparams.src_vocab_size)\n",
    "    gen = (r for r in words if r > 0)\n",
    "\n",
    "    for r in gen:\n",
    "        glove_vecs[r] += 1.\n",
    "    return glove_vecs\n",
    "        \n",
    "\n",
    "def similarity(a, b, semantic=False):\n",
    "    a = to_word_idx(a)\n",
    "    b = to_word_idx(b)\n",
    "    \n",
    "    a = to_glove_vec(a) if semantic else to_binary_vec(a)\n",
    "    b = to_glove_vec(b) if semantic else to_binary_vec(b)\n",
    "    \n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = next_batch(train_x, train_y, False, batch_size=1, beam_width=1)\n",
    "question = from_word_idx(feed_dict.get(encoder_inputs).T[0])\n",
    "answer = from_word_idx(feed_dict.get(target_labels)[0])\n",
    "ir_answers = query_es(question, 2, False)\n",
    "replies_greedy = sess.run(greedy_translations, feed_dict=feed_dict)\n",
    "\n",
    "ir_answer = ir_answers[1][1]\n",
    "greedy_answer = from_word_idx(replies_greedy[0])\n",
    "\n",
    "print(question)\n",
    "print(answer)\n",
    "print()\n",
    "print(ir_answer)\n",
    "print(greedy_answer)\n",
    "\n",
    "print(similarity(answer, ir_answer))\n",
    "print(similarity(answer, greedy_answer))\n",
    "print()\n",
    "print(similarity(answer, ir_answer, True))\n",
    "print(similarity(answer, greedy_answer, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_dataset(x, y, batch_size = hparams.batch_size):\n",
    "    similarity_data = []\n",
    "    for i in tqdm(range(x.shape[0] // batch_size)):\n",
    "        start = i * batch_size\n",
    "        feed_dict = next_batch(x, y, False, \n",
    "                               rand_idx=list(range(start, start + batch_size)),\n",
    "                               batch_size=batch_size,\n",
    "                               beam_width=1)\n",
    "\n",
    "        questions = [from_word_idx(x) for x in feed_dict.get(encoder_inputs).T]\n",
    "        answers = [from_word_idx(x) for x in feed_dict.get(target_labels)]\n",
    "        ir_answers = query_es_bulk(questions, 5, False)\n",
    "        #replies_greedy = sess.run(greedy_translations, feed_dict=feed_dict)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            question = questions[j]\n",
    "            answer = answers[j]\n",
    "            \n",
    "            ir_answer = ir_answers[j]\n",
    "            #greedy_answer = from_word_idx(replies_greedy[j])\n",
    "            \n",
    "            #similarity_data.append([question, greedy_answer, similarity(answer, greedy_answer)])\n",
    "            for ans in ir_answer:\n",
    "                similarity_data.append([question, ans, similarity(answer, ans, True)])\n",
    "                \n",
    "    return pd.DataFrame(similarity_data, columns = ['question', 'answer', 'similarity'])\n",
    "\n",
    "def to_feature_vec(ds, tfidf_vec):\n",
    "    questions_glove = ds.question.progress_apply(lambda x : to_glove_vec(to_word_idx(x)).tolist())\n",
    "    answers_glove = ds.answer.progress_apply(lambda x : to_glove_vec(to_word_idx(x)).tolist())\n",
    "\n",
    "    questions_glove = coo_matrix(np.array(questions_glove.values.tolist())).tocsr()\n",
    "    answers_glove = coo_matrix(np.array(answers_glove.values.tolist())).tocsr()\n",
    "    \n",
    "    questions_bow = tfidf_vec.transform(ds.question)\n",
    "    answers_bow = tfidf_vec.transform(ds.answer)\n",
    "    \n",
    "    features = hstack((questions_bow, answers_bow, \n",
    "                   questions_glove, answers_glove))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_ds = create_similarity_dataset(train_x, train_y, 128)\n",
    "similarity_ds.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "last_idx = int(similarity_ds.shape[0] * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tweet_tokenize)\n",
    "tfidf_vec.fit(similarity_ds[:last_idx].question + ' ' + similarity_ds[:last_idx].answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.Ridge(alpha=1., max_iter=100, solver='auto')\n",
    "\n",
    "train_features = to_feature_vec(similarity_ds[:last_idx], tfidf_vec)\n",
    "test_features = to_feature_vec(similarity_ds[last_idx:], tfidf_vec)\n",
    "\n",
    "regr.fit(train_features, similarity_ds.similarity[:last_idx])\n",
    "\n",
    "# Make predictions using the testing set\n",
    "print(mean_squared_error(similarity_ds.similarity[:last_idx], \n",
    "                             regr.predict(train_features)))\n",
    "\n",
    "print(mean_squared_error(similarity_ds.similarity[last_idx:], \n",
    "                             regr.predict(test_features)))\n",
    "test_cpy = similarity_ds[last_idx:] \n",
    "test_cpy['predicted'] = regr.predict(test_features)\n",
    "\n",
    "del train_features, test_features\n",
    "test_cpy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_idx = int(similarity_ds.shape[0] * 0.8)\n",
    "\n",
    "train_x = similarity_ds.loc[:last_idx, ['question', 'answer']]\n",
    "test_x = similarity_ds.loc[last_idx:, ['question', 'answer']]\n",
    "\n",
    "train_y = similarity_ds.loc[:last_idx, ['similarity']]\n",
    "test_y = similarity_ds.loc[last_idx:, ['similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch_(x, y, train, rand_idx = None, weights = None, \n",
    "               beam_width = hparams.beam_width, batch_size = hparams.batch_size,\n",
    "               go_backwards = False):\n",
    "    if rand_idx is None:\n",
    "        rand_idx = random.sample(list(range(len(x))), batch_size)\n",
    "    ques = np.array(list(map(to_word_idx, x.iloc[rand_idx].question.values)))\n",
    "    ans = np.array(list(map(to_word_idx, x.iloc[rand_idx].answer.values)))\n",
    "    \n",
    "    q_lengths = np.count_nonzero(ques, axis=1)\n",
    "    a_lengths = np.count_nonzero(ans, axis=1)\n",
    "    \n",
    "    feed_dict = {\n",
    "        question_inputs: ques.T,\n",
    "        question_lengths:q_lengths,\n",
    "        targets: y.iloc[rand_idx].values,\n",
    "        answer_inputs: ans.T,\n",
    "        answer_lengths: a_lengths,\n",
    "        keep_prob: (1 - hparams.dropout) if (train) else 1.,\n",
    "    }\n",
    "\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debug purpose.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "targets = tf.placeholder(tf.float32, name='similarties')\n",
    "\n",
    "question_inputs = tf.placeholder(\n",
    "    tf.int32, shape=(None, None), name=\"question_inputs\")\n",
    "answer_inputs = tf.placeholder(\n",
    "    tf.int32, shape=(None, None), name=\"answer_inputs\")\n",
    "keep_prob = tf.placeholder_with_default(1.0, shape=())\n",
    "\n",
    "question_lengths = tf.placeholder(tf.int32, [None], name=\"question_lengths\")\n",
    "answer_lengths = tf.placeholder(tf.int32, [None], name=\"answer_lengths\")\n",
    "\n",
    "shared_embedding = tf.get_variable(\n",
    "        \"shared_embedding\", [hparams.src_vocab_size, hparams.embedding_size])\n",
    "\n",
    "def _rnn_cell(num_units, keep_prob, state_is_tuple = True):\n",
    "    #initializer=tf.glorot_uniform_initializer()\n",
    "    \n",
    "    #Luong init https://arxiv.org/pdf/1508.04025.pdf \n",
    "    initializer=tf.random_uniform_initializer(-0.1, 0.1)\n",
    "    \n",
    "    _cell = (tf.nn.rnn_cell.LSTMCell(num_units, state_is_tuple=state_is_tuple, initializer=initializer, \n",
    "                                    name = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10))))\n",
    "    _cell = tf.contrib.rnn.DropoutWrapper(_cell, output_keep_prob=keep_prob, variational_recurrent=True, dtype=tf.float32)\n",
    "    return _cell\n",
    "\n",
    "def build_rnn_cell(num_units, keep_prob, state_is_tuple = True, layers = 1):\n",
    "    if (layers == 1):\n",
    "        return _rnn_cell(num_units, keep_prob, state_is_tuple)\n",
    "    else:\n",
    "        return tf.contrib.rnn.MultiRNNCell(\n",
    "            [_rnn_cell(num_units, keep_prob, state_is_tuple) for i in range(layers)],\n",
    "            state_is_tuple=state_is_tuple)\n",
    "\n",
    "q_cell = build_rnn_cell(hparams.num_units, keep_prob, layers = 1, state_is_tuple=True)\n",
    "a_cell = build_rnn_cell(hparams.num_units, keep_prob, layers = 1, state_is_tuple=True)\n",
    "\n",
    "_, question_state = tf.nn.dynamic_rnn(\n",
    "        q_cell, tf.nn.embedding_lookup(shared_embedding, question_inputs), \n",
    "        time_major=False, \n",
    "        sequence_length=question_lengths, \n",
    "        dtype=tf.float32)\n",
    "    \n",
    "_, answer_state = tf.nn.dynamic_rnn(\n",
    "        a_cell, tf.nn.embedding_lookup(shared_embedding, answer_inputs), \n",
    "        time_major=False, \n",
    "        sequence_length=answer_lengths, \n",
    "        dtype=tf.float32)\n",
    "\n",
    "concat_state = tf.concat([question_state.h, answer_state.h], -1)\n",
    "\n",
    "outputs = tf.layers.dense(concat_state, 1)\n",
    "\n",
    "train_loss = tf.sqrt(tf.reduce_mean(tf.squared_difference(outputs, targets)))\n",
    "\n",
    "# Train\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Calculate and clip gradients\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(train_loss, params)\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients,\n",
    "                                              hparams.max_gradient_norm)\n",
    "\n",
    "iters_per_epoch = train_x.shape[0] // hparams.batch_size\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.)#hparams.learning_rate)\n",
    "train_op = optimizer.apply_gradients(\n",
    "    zip(clipped_gradients, params), global_step=global_step)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "iters_per_epoch = train_x.shape[0] // hparams.batch_size\n",
    "try:\n",
    "    losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    max_shape = iters_per_epoch * hparams.batch_size\n",
    "    for epoch in range(hparams.max_epochs):\n",
    "        print('\\nStarting epoch = {}/{}'.format(epoch + 1, hparams.max_epochs))\n",
    "        \n",
    "        batch_ids = np.array(list(range(train_x.shape[0])))\n",
    "        np.random.shuffle(batch_ids)\n",
    "        \n",
    "        batch_ids = batch_ids[:max_shape].reshape((-1, hparams.batch_size))\n",
    "        \n",
    "        perc = 0\n",
    "        t_start = time.time()\n",
    "        for i in tqdm(range(iters_per_epoch)):\n",
    "            step = sess.run(global_step)\n",
    "            feed_dict = next_batch_(train_x, train_y, True, rand_idx=None, \n",
    "                                   weights=train_weights, beam_width=1)\n",
    "            _, loss_value = sess.run([train_op, train_loss], feed_dict=feed_dict)\n",
    "            losses.append(loss_value)\n",
    "\n",
    "        t_epoch_end = time.time() - t_start\n",
    "        \n",
    "        t_start = time.time()\n",
    "        for i in tqdm(range(test_x.shape[0] // hparams.batch_size)):\n",
    "            start = i * hparams.batch_size\n",
    "            feed_dict = next_batch_(test_x, test_y, False, rand_idx=list(range(start, start + hparams.batch_size)),\n",
    "                                  weights=None, beam_width=1)\n",
    "            test_loss_value = sess.run([train_loss], feed_dict=feed_dict)\n",
    "            test_losses.append(test_loss_value)\n",
    "    \n",
    "        train_loss_value = np.mean(losses)\n",
    "        test_loss_value = np.mean(test_losses)\n",
    "        \n",
    "        t_test_end = time.time() - t_start\n",
    "        \n",
    "        print('\\nloss = {}, test_loss = {}'\n",
    "              .format(train_loss_value, test_loss_value))\n",
    "        print('Train elapsed {}, Test elapsed {}'.format(t_epoch_end, t_test_end))\n",
    "        \n",
    "        losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nHalting training from keyboard interrupt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cpy = similarity_ds[last_idx:last_idx+128]\n",
    "feed_dict = next_batch_(similarity_ds[last_idx:last_idx+128], similarity_ds[last_idx:last_idx+128].similarity,\n",
    "                               False, rand_idx=None, \n",
    "                               weights=None, beam_width=1,\n",
    "                               batch_size = 128)\n",
    "test_cpy['predicted'] = sess.run(outputs, feed_dict=feed_dict)\n",
    "test_cpy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "554px",
    "left": "1279px",
    "right": "234px",
    "top": "159px",
    "width": "342px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
