{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading en2de_word.txt\n",
      "loading en2de.h5\n",
      "loading en2de.valid.h5\n",
      "seq 1 words: 7943\n",
      "seq 2 words: 2593\n",
      "train shapes: (45582, 60) (45582, 60)\n",
      "valid shapes: (4044, 60) (4044, 60)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_489 (Lambda)             (None, None)         0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_492 (Lambda)             (None, None)         0           lambda_489[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_491 (Lambda)             (None, None)         0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, None, 512)    35840       lambda_491[0][0]                 \n",
      "                                                                 lambda_492[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, None, 512)    4066816     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_157 (Add)                   (None, None, 512)    0           embedding_11[0][0]               \n",
      "                                                                 embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_172 (Dense)               (None, None, 384)    196608      add_157[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_173 (Dense)               (None, None, 384)    196608      add_157[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_493 (Lambda)             (None, None, None)   0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_494 (Lambda)             (None, None, 64)     0           dense_172[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_495 (Lambda)             (None, None, 64)     0           dense_173[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_497 (Lambda)             (None, None, None)   0           lambda_493[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_498 (Lambda)             (None, None, None)   0           lambda_494[0][0]                 \n",
      "                                                                 lambda_495[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_499 (Lambda)             (None, None, None)   0           lambda_497[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_158 (Add)                   (None, None, None)   0           lambda_498[0][0]                 \n",
      "                                                                 lambda_499[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, None)   0           add_158[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_174 (Dense)               (None, None, 384)    196608      add_157[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_126 (Dropout)           (None, None, None)   0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_496 (Lambda)             (None, None, 64)     0           dense_174[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_500 (Lambda)             (None, None, 64)     0           dropout_126[0][0]                \n",
      "                                                                 lambda_496[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_501 (Lambda)             (None, None, 384)    0           lambda_500[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_46 (TimeDistri (None, None, 512)    197120      lambda_501[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_146 (Dropout)           (None, None, 512)    0           time_distributed_46[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_159 (Add)                   (None, None, 512)    0           dropout_146[0][0]                \n",
      "                                                                 add_157[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_71 (LayerNo (None, None, 512)    1024        add_159[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, None, 1024)   525312      layer_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, None, 512)    524800      conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_127 (Dropout)           (None, None, 512)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_160 (Add)                   (None, None, 512)    0           dropout_127[0][0]                \n",
      "                                                                 layer_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_72 (LayerNo (None, None, 512)    1024        add_160[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_176 (Dense)               (None, None, 384)    196608      layer_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_177 (Dense)               (None, None, 384)    196608      layer_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_502 (Lambda)             (None, None, 64)     0           dense_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_503 (Lambda)             (None, None, 64)     0           dense_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_505 (Lambda)             (None, None, None)   0           lambda_493[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_506 (Lambda)             (None, None, None)   0           lambda_502[0][0]                 \n",
      "                                                                 lambda_503[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_507 (Lambda)             (None, None, None)   0           lambda_505[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_161 (Add)                   (None, None, None)   0           lambda_506[0][0]                 \n",
      "                                                                 lambda_507[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, None)   0           add_161[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_178 (Dense)               (None, None, 384)    196608      layer_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_128 (Dropout)           (None, None, None)   0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_504 (Lambda)             (None, None, 64)     0           dense_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_508 (Lambda)             (None, None, 64)     0           dropout_128[0][0]                \n",
      "                                                                 lambda_504[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_509 (Lambda)             (None, None, 384)    0           lambda_508[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_47 (TimeDistri (None, None, 512)    197120      lambda_509[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_147 (Dropout)           (None, None, 512)    0           time_distributed_47[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_162 (Add)                   (None, None, 512)    0           dropout_147[0][0]                \n",
      "                                                                 layer_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_73 (LayerNo (None, None, 512)    1024        add_162[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, None, 1024)   525312      layer_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, None, 512)    524800      conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_129 (Dropout)           (None, None, 512)    0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_163 (Add)                   (None, None, 512)    0           dropout_129[0][0]                \n",
      "                                                                 layer_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_74 (LayerNo (None, None, 512)    1024        add_163[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_180 (Dense)               (None, None, 384)    196608      layer_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_181 (Dense)               (None, None, 384)    196608      layer_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_510 (Lambda)             (None, None, 64)     0           dense_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_511 (Lambda)             (None, None, 64)     0           dense_181[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_513 (Lambda)             (None, None, None)   0           lambda_493[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_514 (Lambda)             (None, None, None)   0           lambda_510[0][0]                 \n",
      "                                                                 lambda_511[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_515 (Lambda)             (None, None, None)   0           lambda_513[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_164 (Add)                   (None, None, None)   0           lambda_514[0][0]                 \n",
      "                                                                 lambda_515[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, None, None)   0           add_164[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_182 (Dense)               (None, None, 384)    196608      layer_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_130 (Dropout)           (None, None, None)   0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_512 (Lambda)             (None, None, 64)     0           dense_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_516 (Lambda)             (None, None, 64)     0           dropout_130[0][0]                \n",
      "                                                                 lambda_512[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_517 (Lambda)             (None, None, 384)    0           lambda_516[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_48 (TimeDistri (None, None, 512)    197120      lambda_517[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_148 (Dropout)           (None, None, 512)    0           time_distributed_48[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_165 (Add)                   (None, None, 512)    0           dropout_148[0][0]                \n",
      "                                                                 layer_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_75 (LayerNo (None, None, 512)    1024        add_165[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, None, 1024)   525312      layer_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, None, 512)    524800      conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_131 (Dropout)           (None, None, 512)    0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_166 (Add)                   (None, None, 512)    0           dropout_131[0][0]                \n",
      "                                                                 layer_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_76 (LayerNo (None, None, 512)    1024        add_166[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_184 (Dense)               (None, None, 384)    196608      layer_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_185 (Dense)               (None, None, 384)    196608      layer_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_518 (Lambda)             (None, None, 64)     0           dense_184[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_519 (Lambda)             (None, None, 64)     0           dense_185[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_521 (Lambda)             (None, None, None)   0           lambda_493[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_522 (Lambda)             (None, None, None)   0           lambda_518[0][0]                 \n",
      "                                                                 lambda_519[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_523 (Lambda)             (None, None, None)   0           lambda_521[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, None, 512)    1327616     lambda_489[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_167 (Add)                   (None, None, None)   0           lambda_522[0][0]                 \n",
      "                                                                 lambda_523[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_170 (Add)                   (None, None, 512)    0           embedding_12[0][0]               \n",
      "                                                                 embedding_10[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_526 (Lambda)             (None, None, None)   0           lambda_489[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_527 (Lambda)             (None, None, None)   0           lambda_489[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, None, None)   0           add_167[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_186 (Dense)               (None, None, 384)    196608      layer_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_188 (Dense)               (None, None, 384)    196608      add_170[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_189 (Dense)               (None, None, 384)    196608      add_170[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_528 (Lambda)             (None, None, None)   0           lambda_526[0][0]                 \n",
      "                                                                 lambda_527[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_132 (Dropout)           (None, None, None)   0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_520 (Lambda)             (None, None, 64)     0           dense_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_530 (Lambda)             (None, None, 64)     0           dense_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_531 (Lambda)             (None, None, 64)     0           dense_189[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_533 (Lambda)             (None, None, None)   0           lambda_528[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_524 (Lambda)             (None, None, 64)     0           dropout_132[0][0]                \n",
      "                                                                 lambda_520[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_534 (Lambda)             (None, None, None)   0           lambda_530[0][0]                 \n",
      "                                                                 lambda_531[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_535 (Lambda)             (None, None, None)   0           lambda_533[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_525 (Lambda)             (None, None, 384)    0           lambda_524[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_171 (Add)                   (None, None, None)   0           lambda_534[0][0]                 \n",
      "                                                                 lambda_535[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_49 (TimeDistri (None, None, 512)    197120      lambda_525[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, None, None)   0           add_171[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_190 (Dense)               (None, None, 384)    196608      add_170[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_149 (Dropout)           (None, None, 512)    0           time_distributed_49[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_134 (Dropout)           (None, None, None)   0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_532 (Lambda)             (None, None, 64)     0           dense_190[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_168 (Add)                   (None, None, 512)    0           dropout_149[0][0]                \n",
      "                                                                 layer_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_536 (Lambda)             (None, None, 64)     0           dropout_134[0][0]                \n",
      "                                                                 lambda_532[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_77 (LayerNo (None, None, 512)    1024        add_168[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_537 (Lambda)             (None, None, 384)    0           lambda_536[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, None, 1024)   525312      layer_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_50 (TimeDistri (None, None, 512)    197120      lambda_537[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, None, 512)    524800      conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_150 (Dropout)           (None, None, 512)    0           time_distributed_50[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_133 (Dropout)           (None, None, 512)    0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_172 (Add)                   (None, None, 512)    0           dropout_150[0][0]                \n",
      "                                                                 add_170[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_169 (Add)                   (None, None, 512)    0           dropout_133[0][0]                \n",
      "                                                                 layer_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_79 (LayerNo (None, None, 512)    1024        add_172[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_78 (LayerNo (None, None, 512)    1024        add_169[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_192 (Dense)               (None, None, 384)    196608      layer_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_193 (Dense)               (None, None, 384)    196608      layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_529 (Lambda)             (None, None, None)   0           lambda_489[0][0]                 \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_538 (Lambda)             (None, None, 64)     0           dense_192[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_539 (Lambda)             (None, None, 64)     0           dense_193[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_541 (Lambda)             (None, None, None)   0           lambda_529[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_542 (Lambda)             (None, None, None)   0           lambda_538[0][0]                 \n",
      "                                                                 lambda_539[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_543 (Lambda)             (None, None, None)   0           lambda_541[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_173 (Add)                   (None, None, None)   0           lambda_542[0][0]                 \n",
      "                                                                 lambda_543[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, None, None)   0           add_173[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_194 (Dense)               (None, None, 384)    196608      layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_135 (Dropout)           (None, None, None)   0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_540 (Lambda)             (None, None, 64)     0           dense_194[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_544 (Lambda)             (None, None, 64)     0           dropout_135[0][0]                \n",
      "                                                                 lambda_540[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_545 (Lambda)             (None, None, 384)    0           lambda_544[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_51 (TimeDistri (None, None, 512)    197120      lambda_545[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_151 (Dropout)           (None, None, 512)    0           time_distributed_51[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_174 (Add)                   (None, None, 512)    0           dropout_151[0][0]                \n",
      "                                                                 layer_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_80 (LayerNo (None, None, 512)    1024        add_174[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, None, 1024)   525312      layer_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, None, 512)    524800      conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_136 (Dropout)           (None, None, 512)    0           conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_175 (Add)                   (None, None, 512)    0           dropout_136[0][0]                \n",
      "                                                                 layer_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_81 (LayerNo (None, None, 512)    1024        add_175[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_196 (Dense)               (None, None, 384)    196608      layer_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_197 (Dense)               (None, None, 384)    196608      layer_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_546 (Lambda)             (None, None, 64)     0           dense_196[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_547 (Lambda)             (None, None, 64)     0           dense_197[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_549 (Lambda)             (None, None, None)   0           lambda_528[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_550 (Lambda)             (None, None, None)   0           lambda_546[0][0]                 \n",
      "                                                                 lambda_547[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_551 (Lambda)             (None, None, None)   0           lambda_549[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_176 (Add)                   (None, None, None)   0           lambda_550[0][0]                 \n",
      "                                                                 lambda_551[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, None, None)   0           add_176[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_198 (Dense)               (None, None, 384)    196608      layer_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_137 (Dropout)           (None, None, None)   0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_548 (Lambda)             (None, None, 64)     0           dense_198[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_552 (Lambda)             (None, None, 64)     0           dropout_137[0][0]                \n",
      "                                                                 lambda_548[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_553 (Lambda)             (None, None, 384)    0           lambda_552[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_52 (TimeDistri (None, None, 512)    197120      lambda_553[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_152 (Dropout)           (None, None, 512)    0           time_distributed_52[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_177 (Add)                   (None, None, 512)    0           dropout_152[0][0]                \n",
      "                                                                 layer_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_82 (LayerNo (None, None, 512)    1024        add_177[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_200 (Dense)               (None, None, 384)    196608      layer_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_201 (Dense)               (None, None, 384)    196608      layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_554 (Lambda)             (None, None, 64)     0           dense_200[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_555 (Lambda)             (None, None, 64)     0           dense_201[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_557 (Lambda)             (None, None, None)   0           lambda_529[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_558 (Lambda)             (None, None, None)   0           lambda_554[0][0]                 \n",
      "                                                                 lambda_555[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_559 (Lambda)             (None, None, None)   0           lambda_557[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_178 (Add)                   (None, None, None)   0           lambda_558[0][0]                 \n",
      "                                                                 lambda_559[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, None, None)   0           add_178[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_202 (Dense)               (None, None, 384)    196608      layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_138 (Dropout)           (None, None, None)   0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_556 (Lambda)             (None, None, 64)     0           dense_202[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_560 (Lambda)             (None, None, 64)     0           dropout_138[0][0]                \n",
      "                                                                 lambda_556[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_561 (Lambda)             (None, None, 384)    0           lambda_560[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_53 (TimeDistri (None, None, 512)    197120      lambda_561[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_153 (Dropout)           (None, None, 512)    0           time_distributed_53[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_179 (Add)                   (None, None, 512)    0           dropout_153[0][0]                \n",
      "                                                                 layer_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_83 (LayerNo (None, None, 512)    1024        add_179[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, None, 1024)   525312      layer_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, None, 512)    524800      conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_139 (Dropout)           (None, None, 512)    0           conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_180 (Add)                   (None, None, 512)    0           dropout_139[0][0]                \n",
      "                                                                 layer_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_84 (LayerNo (None, None, 512)    1024        add_180[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_204 (Dense)               (None, None, 384)    196608      layer_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_205 (Dense)               (None, None, 384)    196608      layer_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_562 (Lambda)             (None, None, 64)     0           dense_204[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_563 (Lambda)             (None, None, 64)     0           dense_205[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_565 (Lambda)             (None, None, None)   0           lambda_528[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_566 (Lambda)             (None, None, None)   0           lambda_562[0][0]                 \n",
      "                                                                 lambda_563[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_567 (Lambda)             (None, None, None)   0           lambda_565[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_181 (Add)                   (None, None, None)   0           lambda_566[0][0]                 \n",
      "                                                                 lambda_567[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, None, None)   0           add_181[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_206 (Dense)               (None, None, 384)    196608      layer_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_140 (Dropout)           (None, None, None)   0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_564 (Lambda)             (None, None, 64)     0           dense_206[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_568 (Lambda)             (None, None, 64)     0           dropout_140[0][0]                \n",
      "                                                                 lambda_564[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_569 (Lambda)             (None, None, 384)    0           lambda_568[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_54 (TimeDistri (None, None, 512)    197120      lambda_569[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_154 (Dropout)           (None, None, 512)    0           time_distributed_54[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_182 (Add)                   (None, None, 512)    0           dropout_154[0][0]                \n",
      "                                                                 layer_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_85 (LayerNo (None, None, 512)    1024        add_182[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_208 (Dense)               (None, None, 384)    196608      layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_209 (Dense)               (None, None, 384)    196608      layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_570 (Lambda)             (None, None, 64)     0           dense_208[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_571 (Lambda)             (None, None, 64)     0           dense_209[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_573 (Lambda)             (None, None, None)   0           lambda_529[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_574 (Lambda)             (None, None, None)   0           lambda_570[0][0]                 \n",
      "                                                                 lambda_571[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_575 (Lambda)             (None, None, None)   0           lambda_573[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_183 (Add)                   (None, None, None)   0           lambda_574[0][0]                 \n",
      "                                                                 lambda_575[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, None, None)   0           add_183[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_210 (Dense)               (None, None, 384)    196608      layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_141 (Dropout)           (None, None, None)   0           activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_572 (Lambda)             (None, None, 64)     0           dense_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_576 (Lambda)             (None, None, 64)     0           dropout_141[0][0]                \n",
      "                                                                 lambda_572[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_577 (Lambda)             (None, None, 384)    0           lambda_576[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_55 (TimeDistri (None, None, 512)    197120      lambda_577[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_155 (Dropout)           (None, None, 512)    0           time_distributed_55[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_184 (Add)                   (None, None, 512)    0           dropout_155[0][0]                \n",
      "                                                                 layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_86 (LayerNo (None, None, 512)    1024        add_184[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, None, 1024)   525312      layer_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)              (None, None, 512)    524800      conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_142 (Dropout)           (None, None, 512)    0           conv1d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_185 (Add)                   (None, None, 512)    0           dropout_142[0][0]                \n",
      "                                                                 layer_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_87 (LayerNo (None, None, 512)    1024        add_185[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_212 (Dense)               (None, None, 384)    196608      layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_213 (Dense)               (None, None, 384)    196608      layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_578 (Lambda)             (None, None, 64)     0           dense_212[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_579 (Lambda)             (None, None, 64)     0           dense_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_581 (Lambda)             (None, None, None)   0           lambda_528[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_582 (Lambda)             (None, None, None)   0           lambda_578[0][0]                 \n",
      "                                                                 lambda_579[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_583 (Lambda)             (None, None, None)   0           lambda_581[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_186 (Add)                   (None, None, None)   0           lambda_582[0][0]                 \n",
      "                                                                 lambda_583[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, None, None)   0           add_186[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_214 (Dense)               (None, None, 384)    196608      layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_143 (Dropout)           (None, None, None)   0           activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_580 (Lambda)             (None, None, 64)     0           dense_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_584 (Lambda)             (None, None, 64)     0           dropout_143[0][0]                \n",
      "                                                                 lambda_580[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_585 (Lambda)             (None, None, 384)    0           lambda_584[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_56 (TimeDistri (None, None, 512)    197120      lambda_585[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_156 (Dropout)           (None, None, 512)    0           time_distributed_56[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_187 (Add)                   (None, None, 512)    0           dropout_156[0][0]                \n",
      "                                                                 layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_88 (LayerNo (None, None, 512)    1024        add_187[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_216 (Dense)               (None, None, 384)    196608      layer_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_217 (Dense)               (None, None, 384)    196608      layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_586 (Lambda)             (None, None, 64)     0           dense_216[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_587 (Lambda)             (None, None, 64)     0           dense_217[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_589 (Lambda)             (None, None, None)   0           lambda_529[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_590 (Lambda)             (None, None, None)   0           lambda_586[0][0]                 \n",
      "                                                                 lambda_587[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_591 (Lambda)             (None, None, None)   0           lambda_589[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_188 (Add)                   (None, None, None)   0           lambda_590[0][0]                 \n",
      "                                                                 lambda_591[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, None, None)   0           add_188[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_218 (Dense)               (None, None, 384)    196608      layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_144 (Dropout)           (None, None, None)   0           activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_588 (Lambda)             (None, None, 64)     0           dense_218[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_592 (Lambda)             (None, None, 64)     0           dropout_144[0][0]                \n",
      "                                                                 lambda_588[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_593 (Lambda)             (None, None, 384)    0           lambda_592[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_57 (TimeDistri (None, None, 512)    197120      lambda_593[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_157 (Dropout)           (None, None, 512)    0           time_distributed_57[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_189 (Add)                   (None, None, 512)    0           dropout_157[0][0]                \n",
      "                                                                 layer_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_89 (LayerNo (None, None, 512)    1024        add_189[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)              (None, None, 1024)   525312      layer_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)              (None, None, 512)    524800      conv1d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_145 (Dropout)           (None, None, 512)    0           conv1d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_190 (Add)                   (None, None, 512)    0           dropout_145[0][0]                \n",
      "                                                                 layer_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_90 (LayerNo (None, None, 512)    1024        add_190[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_58 (TimeDistri (None, None, 2593)   1327616     layer_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_490 (Lambda)             (None, None)         0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_594 (Lambda)             ()                   0           time_distributed_58[0][0]        \n",
      "                                                                 lambda_490[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 24,622,592\n",
      "Trainable params: 24,586,752\n",
      "Non-trainable params: 35,840\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "new model\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import os, sys\n",
    "import dataloader as dd\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "importlib.reload(dd)\n",
    "\n",
    "itokens, otokens = dd.MakeS2SDict('en2de.s2s.txt', dict_file='en2de_word.txt', min_freq=4)\n",
    "Xtrain, Ytrain = dd.MakeS2SData(\n",
    "    'en2de.s2s.txt', itokens, otokens, h5_file='en2de.h5', max_len=60)\n",
    "Xvalid, Yvalid = dd.MakeS2SData(\n",
    "    'en2de.s2s.valid.txt', itokens, otokens, h5_file='en2de.valid.h5', max_len=60)\n",
    "\n",
    "print('seq 1 words:', itokens.num())\n",
    "print('seq 2 words:', otokens.num())\n",
    "print('train shapes:', Xtrain.shape, Ytrain.shape)\n",
    "print('valid shapes:', Xvalid.shape, Yvalid.shape)\n",
    "\n",
    "'''\n",
    "from rnn_s2s import RNNSeq2Seq\n",
    "s2s = RNNSeq2Seq(itokens,otokens, 256)\n",
    "s2s.compile('rmsprop')\n",
    "s2s.model.fit([Xtrain, Ytrain], None, batch_size=64, epochs=30, validation_data=([Xvalid, Yvalid], None))\n",
    "'''\n",
    "\n",
    "from transformer import Transformer, LRSchedulerPerStep, LRSchedulerPerEpoch\n",
    "\n",
    "iters_per_epoch = int(np.ceil(Xtrain.shape[0] / 64))\n",
    "\n",
    "l_rate = 0.001\n",
    "class LRSchedulerExponentialDecay(Callback):\n",
    "    def __init__(self, d_model, warmup=4000):\n",
    "        self.step_num = 0\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs = None):\n",
    "        self.step_num += 1\n",
    "        lr =  l_rate * (0.99 ** (self.step_num // iters_per_epoch))\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "# d_model = 256\n",
    "# s2s = Transformer(itokens, otokens, len_limit=300, d_model=d_model, d_inner_hid=512, \\\n",
    "#        n_head=8, d_k=64, d_v=64, layers=6, dropout=0.1)\n",
    "\n",
    "d_model = 512\n",
    "s2s = Transformer(itokens, otokens, len_limit=70, d_model=d_model, d_inner_hid=1024, \\\n",
    "         n_head=6, d_k=64, d_v=64, layers=4, dropout=0.1)\n",
    "\n",
    "lr_scheduler = LRSchedulerPerStep(\n",
    "     d_model, 4000)  # there is a warning that it is slow, however, it's ok.\n",
    "#lr_scheduler = LRSchedulerExponentialDecay(d_model, 4000)  # this scheduler only update lr per epoch\n",
    "model_saver = ModelCheckpoint(\n",
    "    'en2de.model.h5', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "s2s.compile(Adam(l_rate , 0.9, 0.999, epsilon=1e-8))\n",
    "s2s.model.summary()\n",
    "try:\n",
    "    s2s.model.load_weights('en2de.model.h5')\n",
    "except:\n",
    "    print('\\n\\nnew model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45582 samples, validate on 4044 samples\n",
      "Epoch 1/15\n",
      "45582/45582 [==============================] - 263s 6ms/step - loss: 4.7247 - ppl: 289.9156 - accu: 0.2163 - val_loss: 4.0402 - val_ppl: 57.5753 - val_accu: 0.2697\n",
      "Epoch 2/15\n",
      "45582/45582 [==============================] - 246s 5ms/step - loss: 2.5877 - ppl: 14.1564 - accu: 0.4928 - val_loss: 2.9146 - val_ppl: 18.7585 - val_accu: 0.4206\n",
      "Epoch 3/15\n",
      "45582/45582 [==============================] - 245s 5ms/step - loss: 2.0390 - ppl: 7.8051 - accu: 0.5659 - val_loss: 2.5284 - val_ppl: 12.7378 - val_accu: 0.4724\n",
      "Epoch 4/15\n",
      "45582/45582 [==============================] - 246s 5ms/step - loss: 1.8314 - ppl: 6.3188 - accu: 0.5944 - val_loss: 2.3746 - val_ppl: 10.9413 - val_accu: 0.4947\n",
      "Epoch 5/15\n",
      "45582/45582 [==============================] - 243s 5ms/step - loss: 1.7164 - ppl: 5.6210 - accu: 0.6114 - val_loss: 2.3065 - val_ppl: 10.2070 - val_accu: 0.5038\n",
      "Epoch 6/15\n",
      "45582/45582 [==============================] - 241s 5ms/step - loss: 1.6459 - ppl: 5.2373 - accu: 0.6209 - val_loss: 2.2065 - val_ppl: 9.2416 - val_accu: 0.5180\n",
      "Epoch 7/15\n",
      "45582/45582 [==============================] - 241s 5ms/step - loss: 1.5660 - ppl: 4.8300 - accu: 0.6327 - val_loss: 2.1630 - val_ppl: 8.8521 - val_accu: 0.5281\n",
      "Epoch 8/15\n",
      "45582/45582 [==============================] - 246s 5ms/step - loss: 1.5020 - ppl: 4.5267 - accu: 0.6416 - val_loss: 2.1151 - val_ppl: 8.4391 - val_accu: 0.5355\n",
      "Epoch 9/15\n",
      "45582/45582 [==============================] - 248s 5ms/step - loss: 1.4494 - ppl: 4.2907 - accu: 0.6499 - val_loss: 2.0920 - val_ppl: 8.2426 - val_accu: 0.5386\n",
      "Epoch 10/15\n",
      "45582/45582 [==============================] - 250s 5ms/step - loss: 1.4063 - ppl: 4.1106 - accu: 0.6559 - val_loss: 2.0619 - val_ppl: 7.9945 - val_accu: 0.5439\n",
      "Epoch 11/15\n",
      "45582/45582 [==============================] - 252s 6ms/step - loss: 1.3677 - ppl: 3.9530 - accu: 0.6620 - val_loss: 2.0437 - val_ppl: 7.8499 - val_accu: 0.5471\n",
      "Epoch 12/15\n",
      "45582/45582 [==============================] - 249s 5ms/step - loss: 1.3291 - ppl: 3.8029 - accu: 0.6684 - val_loss: 2.0416 - val_ppl: 7.8475 - val_accu: 0.5498\n",
      "Epoch 13/15\n",
      "45582/45582 [==============================] - 251s 6ms/step - loss: 1.2932 - ppl: 3.6658 - accu: 0.6740 - val_loss: 2.0384 - val_ppl: 7.8317 - val_accu: 0.5532\n",
      "Epoch 14/15\n",
      "45582/45582 [==============================] - 250s 5ms/step - loss: 1.2641 - ppl: 3.5612 - accu: 0.6787 - val_loss: 2.0437 - val_ppl: 7.8728 - val_accu: 0.5535\n",
      "Epoch 15/15\n",
      "45582/45582 [==============================] - 249s 5ms/step - loss: 1.2330 - ppl: 3.4507 - accu: 0.6839 - val_loss: 2.0330 - val_ppl: 7.7915 - val_accu: 0.5550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f50435da908>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2s.model.fit([Xtrain, Ytrain], None, batch_size=64, epochs=15, \\\n",
    "    validation_data=([Xvalid, Yvalid], None), \\\n",
    "    callbacks=[lr_scheduler, model_saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library versions:\n",
      "tensorflow:1.7.0\n",
      "pandas:0.22.0\n",
      "numpy:1.14.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003c044b4d484e1c80d30cbb57579a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import importlib\n",
    "\n",
    "import customersupport.common\n",
    "import customersupport.evaluation\n",
    "import customersupport.evaluation.eval\n",
    "\n",
    "print('Library versions:')\n",
    "\n",
    "import tensorflow as tf\n",
    "print('tensorflow:{}'.format(tf.__version__))\n",
    "import pandas as pd\n",
    "print('pandas:{}'.format(pd.__version__))\n",
    "import numpy as np\n",
    "print('numpy:{}'.format(np.__version__))\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm  # Special jupyter notebook progress bar\n",
    "\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "from customersupport.common.vocab import VocabHolder\n",
    "from customersupport.common.dataset import CustomerSupportDataset\n",
    "\n",
    "from customersupport.evaluation.eval import *#evaluate_words_index, format_metrics, get_evaluation_conf, strip_punkt\n",
    "import customersupport.common.utils\n",
    "\n",
    "importlib.reload(customersupport.common.vocab)\n",
    "importlib.reload(customersupport.common.dataset)\n",
    "importlib.reload(customersupport.evaluation)\n",
    "importlib.reload(customersupport.evaluation.eval)\n",
    "importlib.reload(customersupport.common.utils)\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "tqdm().pandas()  # Enable tracking of progress in dataframe `apply` calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/momchil/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# 8192 - large enough for demonstration, larger values make network training slower\n",
    "MAX_VOCAB_SIZE = 2**14\n",
    "\n",
    "# seq2seq generally relies on fixed length message vectors - longer messages provide more info\n",
    "# but result in slower training and larger networks\n",
    "#MAX_MESSAGE_LEN = 50\n",
    "\n",
    "hparams = tf.contrib.training.HParams(\n",
    "    # Larger batch sizes generally reach the average response faster, but small batch sizes are\n",
    "    # required for the model to learn nuanced responses.  Also, GPU memory limits max batch size.\n",
    "    batch_size=128,\n",
    "    encoder_length=60,\n",
    "    decoder_length=60,\n",
    "    # Embedding size for whole messages, same trade off as word embeddings\n",
    "    num_units=256,\n",
    "    src_vocab_size=MAX_VOCAB_SIZE,\n",
    "    # Embedding size for words - gives a trade off between expressivity of words and network size\n",
    "    embedding_size=200,\n",
    "    tgt_vocab_size=MAX_VOCAB_SIZE,\n",
    "    # Helps regularize network and prevent overfitting.\n",
    "    # High learning rate helps model reach average response faster, but can make it hard to\n",
    "    # converge on nuanced responses\n",
    "    learning_rate=1e-03,  #0.0005,\n",
    "    max_gradient_norm=5.0,\n",
    "    beam_width=10,\n",
    "    use_attention=True,\n",
    "    enc_num_layers=2,\n",
    "    dec_num_layers=2,\n",
    "    cell_type='bi',\n",
    "    rnn_type='gru',\n",
    "    attention_architecture='gnmt',\n",
    "    max_epochs=30,\n",
    "    dropout=0.2,\n",
    "    use_glove=True,\n",
    "    l2_reg=0.,\n",
    "    glove_path=\n",
    "    '/home/momchil/Storage/Projects/Python/Data/glove.twitter.27B/glove.twitter.27B.200d.txt',\n",
    "    tweets_path=\n",
    "    '/home/momchil/Storage/Projects/Python/Data/customer-support-on-twitter/twcs-conv_ids_clean.csv',\n",
    "    # Ngram count for ROUGE and BLEU\n",
    "    max_order=2,\n",
    "    train_size=0.8,\n",
    "    decay_rate=0.99,\n",
    "    train_time_diff=5.0,\n",
    "    first_day=0,\n",
    "    last_day=60,\n",
    "    evaluation_metrics=[\n",
    "        \"bleu\", \"rouge_l\", \"embedding_average\", \"vector_extrema\",\n",
    "        \"greedy_matching\"\n",
    "    ],\n",
    "    training_metrics=[\n",
    "        \"bleu\", \"rouge_l\", \"embedding_average\", \"vector_extrema\",\n",
    "        \"greedy_matching\"\n",
    "    ],\n",
    "    companies=['AppleSupport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done support_author (984679, 9)\n",
      "Replacing anonymized screen names in X...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d09a0bf0bd4e05b3ebf65ce75558a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=105179), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing anonymized screen names in Y...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b8c2f9a073425b8f8783d978f1dca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=105179), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 3min 8s, sys: 923 ms, total: 3min 9s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cs_data = CustomerSupportDataset(hparams)\n",
    "\n",
    "#& (y_text.str.contains('help') ^ True)\n",
    "cs_data.process_utterances(['direct message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded glove\n",
      "Loaded w2v\n",
      "Fitting CountVectorizer on X and Y text data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466dbba5b00440d6b62cd363dec75635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=49626), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of known words 13794\n",
      "Learned vocab of 16384 items.\n",
      "Calculating word indexes for X...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312649f95144428e8eceaf438caf8381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=49626), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating word indexes for Y...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0e3d565f1f466a8eb51afaba0b4a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=49626), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data of shape (45582, 60) and test data of shape (4044, 60).\n",
      "count    45582.000000\n",
      "mean         1.000000\n",
      "std          0.141677\n",
      "min          0.740038\n",
      "25%          0.883758\n",
      "50%          1.021893\n",
      "75%          1.097074\n",
      "max          1.286219\n",
      "dtype: float64\n",
      "count    4044.000000\n",
      "mean        1.000000\n",
      "std         0.014701\n",
      "min         0.972407\n",
      "25%         0.988713\n",
      "50%         1.001299\n",
      "75%         1.011627\n",
      "max         1.022508\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "voc_holder = VocabHolder(hparams)\n",
    "analyzer = voc_holder.fit(cs_data.x_text, cs_data.y_text, hparams.src_vocab_size)\n",
    "\n",
    "cs_data.text_to_vec(hparams, voc_holder)\n",
    "cs_data.train_test_split(hparams, do_random=False)\n",
    "\n",
    "train_x = cs_data.x_text.iloc[list(cs_data.train_idx)].dropna()\n",
    "train_y = cs_data.y_text.iloc[list(cs_data.train_idx)].dropna()\n",
    "\n",
    "test_x = cs_data.x_text.iloc[list(cs_data.test_idx)].dropna()\n",
    "test_y = cs_data.y_text.iloc[list(cs_data.test_idx)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf1d2acbf7444a9bb36b4408b8a2dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4044), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM when allocating tensor with shape[18,7098,7098] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: lambda_604/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_600/Reshape_1, lambda_601/Reshape_1)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: layer_normalization_78_1/add_1/_6913 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1298_layer_normalization_78_1/add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'lambda_604/MatMul', defined at:\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-913e1f9df0a9>\", line 9, in <module>\n",
      "    a_text = s2s.decode_sequence_fast(question, delimiter=' ')\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 329, in decode_sequence_fast\n",
      "    if self.decode_model is None: self.make_fast_decode_model()\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 317, in make_fast_decode_model\n",
      "    enc_output = self.encoder(src_seq, src_pos)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 180, in __call__\n",
      "    x, att = enc_layer(x, mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 129, in __call__\n",
      "    output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 86, in __call__\n",
      "    head, attn = self.attention(qs, ks, vs, mask=mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 33, in __call__\n",
      "    attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/engine/topology.py\", line 617, in __call__\n",
      "    output = self.call(inputs, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/layers/core.py\", line 663, in call\n",
      "    return self.function(inputs, **arguments)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 33, in <lambda>\n",
      "    attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1151, in batch_dot\n",
      "    out = tf.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 2071, in matmul\n",
      "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1236, in batch_mat_mul\n",
      "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[18,7098,7098] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: lambda_604/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_600/Reshape_1, lambda_601/Reshape_1)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: layer_normalization_78_1/add_1/_6913 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1298_layer_normalization_78_1/add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "OOM when allocating tensor with shape[18,7036,7036] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: lambda_604/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_600/Reshape_1, lambda_601/Reshape_1)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: layer_normalization_78_1/add_1/_6913 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1298_layer_normalization_78_1/add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'lambda_604/MatMul', defined at:\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-913e1f9df0a9>\", line 9, in <module>\n",
      "    a_text = s2s.decode_sequence_fast(question, delimiter=' ')\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 329, in decode_sequence_fast\n",
      "    if self.decode_model is None: self.make_fast_decode_model()\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 317, in make_fast_decode_model\n",
      "    enc_output = self.encoder(src_seq, src_pos)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 180, in __call__\n",
      "    x, att = enc_layer(x, mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 129, in __call__\n",
      "    output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 86, in __call__\n",
      "    head, attn = self.attention(qs, ks, vs, mask=mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 33, in __call__\n",
      "    attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/engine/topology.py\", line 617, in __call__\n",
      "    output = self.call(inputs, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/layers/core.py\", line 663, in call\n",
      "    return self.function(inputs, **arguments)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 33, in <lambda>\n",
      "    attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1151, in batch_dot\n",
      "    out = tf.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 2071, in matmul\n",
      "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1236, in batch_mat_mul\n",
      "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[18,7036,7036] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: lambda_604/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_600/Reshape_1, lambda_601/Reshape_1)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: layer_normalization_78_1/add_1/_6913 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1298_layer_normalization_78_1/add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM when allocating tensor with shape[18,7235,7235] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: lambda_604/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_600/Reshape_1, lambda_601/Reshape_1)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: layer_normalization_78_1/add_1/_6913 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1298_layer_normalization_78_1/add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'lambda_604/MatMul', defined at:\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-913e1f9df0a9>\", line 9, in <module>\n",
      "    a_text = s2s.decode_sequence_fast(question, delimiter=' ')\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 329, in decode_sequence_fast\n",
      "    if self.decode_model is None: self.make_fast_decode_model()\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 317, in make_fast_decode_model\n",
      "    enc_output = self.encoder(src_seq, src_pos)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 180, in __call__\n",
      "    x, att = enc_layer(x, mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 129, in __call__\n",
      "    output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 86, in __call__\n",
      "    head, attn = self.attention(qs, ks, vs, mask=mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 33, in __call__\n",
      "    attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/engine/topology.py\", line 617, in __call__\n",
      "    output = self.call(inputs, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/layers/core.py\", line 663, in call\n",
      "    return self.function(inputs, **arguments)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 33, in <lambda>\n",
      "    attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1151, in batch_dot\n",
      "    out = tf.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 2071, in matmul\n",
      "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1236, in batch_mat_mul\n",
      "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[18,7235,7235] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: lambda_604/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_600/Reshape_1, lambda_601/Reshape_1)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: layer_normalization_78_1/add_1/_6913 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1298_layer_normalization_78_1/add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "OOM when allocating tensor with shape[18,7167,7167] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: lambda_604/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_600/Reshape_1, lambda_601/Reshape_1)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: layer_normalization_78_1/add_1/_6913 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1298_layer_normalization_78_1/add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'lambda_604/MatMul', defined at:\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-913e1f9df0a9>\", line 9, in <module>\n",
      "    a_text = s2s.decode_sequence_fast(question, delimiter=' ')\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 329, in decode_sequence_fast\n",
      "    if self.decode_model is None: self.make_fast_decode_model()\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 317, in make_fast_decode_model\n",
      "    enc_output = self.encoder(src_seq, src_pos)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 180, in __call__\n",
      "    x, att = enc_layer(x, mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 129, in __call__\n",
      "    output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 86, in __call__\n",
      "    head, attn = self.attention(qs, ks, vs, mask=mask)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 33, in __call__\n",
      "    attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/engine/topology.py\", line 617, in __call__\n",
      "    output = self.call(inputs, **kwargs)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/layers/core.py\", line 663, in call\n",
      "    return self.function(inputs, **arguments)\n",
      "  File \"/mnt/storage/Projects/Python/Customer Support/customer-support-chatbot/attention-is-all-you-need-keras/transformer.py\", line 33, in <lambda>\n",
      "    attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1151, in batch_dot\n",
      "    out = tf.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 2071, in matmul\n",
      "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1236, in batch_mat_mul\n",
      "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/momchil/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[18,7167,7167] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: lambda_604/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_600/Reshape_1, lambda_601/Reshape_1)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: layer_normalization_78_1/add_1/_6913 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1298_layer_normalization_78_1/add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "hypothesis = []\n",
    "\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    ref = test_y.iloc[i]\n",
    "    question = customersupport.common.utils.tweet_tokenize(test_x.iloc[i])\n",
    "    #voc_holder.from_word_idx(voc_holder.to_word_idx(, -1))[:297]\n",
    "    try:\n",
    "        a_text = s2s.beam_search(question, 3, delimiter=' ')[0][0]\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        a_text = ''\n",
    "\n",
    "    #references.append(strip_punkt(voc_holder.to_word_idx(ref, -1), eval_conf.voc_holder.reverse_vocab))\n",
    "    #hypothesis.append(strip_punkt(voc_holder.to_word_idx(a_text, -1), eval_conf.voc_holder.reverse_vocab))\n",
    "    \n",
    "    r = voc_holder.to_word_idx(ref, -1)\n",
    "    h = voc_holder.to_word_idx(a_text, -1)\n",
    "    references.append(r[r.nonzero()])\n",
    "    hypothesis.append(h[h.nonzero()])\n",
    "\n",
    "    \n",
    "references = np.array(references)\n",
    "hypothesis = np.array(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = np.array(references)\n",
    "hypothesis = np.array(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2898\n",
      "2901\n",
      "2959\n",
      "2962\n"
     ]
    }
   ],
   "source": [
    "for i, r in enumerate(hypothesis):\n",
    "    if (len(r) == 0):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([0]), list([0]), list([0]), list([0])], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis[[2898, 2901, 2959, 2962]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU@2: 12.517067570165455\n",
      "Embedding Average: 76.93455490984044\n",
      "Greedy Matching: 30.129827156763795\n",
      "ROUGE_L: 22.763979193134194\n",
      "Vector Extrema: 37.125948048069475\n"
     ]
    }
   ],
   "source": [
    "eval_conf = get_evaluation_conf(None, hparams, None, None, voc_holder)\n",
    "evaluation = evaluate_words_index(references, hypothesis, eval_conf, hparams.evaluation_metrics, True)\n",
    "print(format_metrics(evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Question': test_x, 'Reference': test_y, 'Hypothesis':list(map(voc_holder.from_word_idx, hypothesis))}, columns = [\"Question\", \"Reference\", \"Hypothesis\"])\n",
    "\n",
    "df.to_csv('/home/momchil/Desktop/transformer_all_dict.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "501px",
    "left": "1485px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
